import torch
from transformers import RobertaTokenizer, RobertaForSequenceClassification
import pandas as pd

# Assuming your DataFrame is named 'df' and the comment column is 'comment' and the label column is 'department'
comments = df['comment'].tolist()
labels = df['department'].tolist()

tokenizer = RobertaTokenizer.from_pretrained('roberta-base')
tokenized_inputs = tokenizer.batch_encode_plus(
    comments,
    truncation=True,
    padding=True,
    return_tensors='pt'
)

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encoded_inputs, labels):
        self.input_ids = encoded_inputs['input_ids']
        self.attention_mask = encoded_inputs['attention_mask']
        self.labels = labels

    def __getitem__(self, idx):
        return {
            'input_ids': self.input_ids[idx],
            'attention_mask': self.attention_mask[idx],
            'labels': self.labels[idx]
        }

    def __len__(self):
        return len(self.labels)

dataset = CustomDataset(tokenized_inputs, labels)

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])

model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

for epoch in range(3):  # Adjust the number of epochs as needed
    model.train()
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

    model.eval()
    with torch.no_grad():
        val_loss = 0
        for batch in val_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            val_loss += outputs.loss.item()

    print(f'Epoch {epoch+1}: Val Loss = {val_loss/len(val_loader):.4f}')


torch.save(model.state_dict(), 'roberta_model.pth')
model.load_state_dict(torch.load('roberta_model.pth'))
model.eval()

 def predict_department(comment):
    inputs = tokenizer.encode_plus(
        comment,
        truncation=True,
        padding=True,
        return_tensors='pt'
    )
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits

    predicted_label = torch.argmax(logits, dim=1).item()
    predicted_department = label_mapping[predicted_label]  # Map the label index to the actual department name
    return predicted_department

# Example usage:
new_comment = "This is a new comment"
predicted_department = predict_department(new_comment)
print(f"Predicted Department: {predicted_department}")
                     
                     
                      
