pip install torch
pip install transformers
import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch

# Load the dataframe
df = pd.read_csv('your_dataframe.csv')

# Split the data into train and test sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Extract the comments and department columns
train_comments = train_df['comments'].tolist()
train_labels = train_df['department'].tolist()

test_comments = test_df['comments'].tolist()
test_labels = test_df['department'].tolist()


model_name = "distilbert-base-uncased"  # You can use any other suitable pre-trained model

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load the model
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(df['department'].unique()))


# Tokenize the training comments
train_encodings = tokenizer(train_comments, truncation=True, padding=True)

# Tokenize the test comments
test_encodings = tokenizer(test_comments, truncation=True, padding=True)

# Convert the labels to tensors
train_labels = torch.tensor(train_labels)
test_labels = torch.tensor(test_labels)


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_encodings,
    eval_dataset=test_encodings,
    compute_metrics=lambda preds, labels: {"accuracy": (preds.argmax(-1) == labels).float().mean()}
)


trainer.train()
trainer.evaluate()


# Example input
new_comments = ["This is a new comment.", "Another comment for prediction."]

# Tokenize the new comments
new_comment_encodings = tokenizer(new_comments, truncation=True, padding=True)

# Make predictions
predictions = trainer.predict(new_comment_encodings)
predicted_labels = predictions.predictions.argmax(axis=1)

# Convert the predicted labels to department names
department_names = [df['department'].unique()[label] for label in predicted_labels]







pip install torch
pip install transformers
pip install pandas
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
# Load the dataframe
df = pd.read_csv('your_dataframe.csv')

# Split the data into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(df['comments'], df['department'], test_size=0.2, random_state=42)

# Initialize the tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)

# Tokenize the input texts
train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)

# Create PyTorch tensors for the input encodings and labels
train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),
                              torch.tensor(train_encodings['attention_mask']),
                              torch.tensor(train_labels.tolist()))
test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),
                             torch.tensor(test_encodings['attention_mask']),
                             torch.tensor(test_labels.tolist()))

model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(df['department'].unique()))

# Create a DataLoader for the training set
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)

# Set the device to GPU if available, otherwise use CPU
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Define the optimizer and learning rate
optimizer = AdamW(model.parameters(), lr=1e-5)

# Training loop
epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()
        loss.backward()
        optimizer.step()

    avg_loss = total_loss / len(train_dataloader)
    print(f'Epoch: {epoch+1}, Average Loss: {avg_loss:.4f}')

# Save the trained model
model.save_pretrained('text_classification_model')

# Create a DataLoader for the test set
test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Evaluation loop
model.eval()
eval_loss = 0
predictions = []

for batch in test_dataloader:
    input_ids = batch[0].to(device)
    attention_mask = batch[1].to(device)
    labels = batch[2].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        logits = outputs.logits

    eval_loss += loss.item()

    predicted_labels = torch.argmax(logits, dim=1)
    predictions.extend(predicted_labels.cpu().numpy())

avg_eval_loss = eval_loss / len(test_dataloader)
classification_rep = classification_report(test_labels.tolist(), predictions)
print(f'Average Evaluation Loss: {avg_eval_loss:.4f}')
print(f'Classification Report:\n{classification_rep}')






import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
# Load data into a DataFrame
data = pd.read_csv('data.csv')

# Split data into training and testing datasets
train_texts, test_texts, train_labels, test_labels = train_test_split(data['comment'], data['department'], test_size=0.2, random_state=42)
# Load the BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

# Tokenize the training texts
train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)

# Tokenize the testing texts
test_encodings = tokenizer(list(test_texts), truncation=True, padding=True)

# Convert the tokenized inputs and labels into PyTorch tensors
train_dataset = TensorDataset(torch.tensor(train_encodings['input_ids']),
                              torch.tensor(train_encodings['attention_mask']),
                              torch.tensor(train_labels))

test_dataset = TensorDataset(torch.tensor(test_encodings['input_ids']),
                             torch.tensor(test_encodings['attention_mask']),
                             torch.tensor(test_labels))

# Set the batch size and create the data loaders
batch_size = 16

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pretrained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_labels)

# Set the optimizer and learning rate
optimizer = AdamW(model.parameters(), lr=1e-5)

# Set the number of training epochs
num_epochs = 5

# Fine-tune the model
for epoch in range(num_epochs):
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1],
                  'labels': batch[2]}
        outputs = model(**inputs)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

model.eval()
predictions = []
for batch in test_loader:
    with torch.no_grad():
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1],
                  'labels': batch[2]}
        outputs = model(**inputs)
        logits = outputs.logits
        predictions.extend(torch.argmax(logits, dim=1).tolist())

# Calculate accuracy
correct = sum(predictions[i] == test_labels[i] for i in range(len(predictions)))
accuracy = correct / len(predictions)
print(f"Accuracy: {accuracy}")

# Tokenize new comments
new_comments = ['New comment 1', 'New comment 2', ...]
new_encodings = tokenizer(list(new_comments), truncation=True, padding=True)

# Create a dataset for new comments
new_dataset = TensorDataset(torch.tensor(new_encodings['input_ids']),
                            torch.tensor(new_encodings['attention_mask']))

# Create a data loader for new comments
new_loader = DataLoader(new_dataset, batch_size=batch_size, shuffle=False)

# Make predictions for new comments
model.eval()
predictions = []
for batch in new_loader:
    with torch.no_grad():
        inputs = {'input_ids': batch[0],
                  'attention_mask': batch[1]}
        outputs = model(**inputs)
        logits = outputs.logits
        predictions.extend(torch.argmax(logits, dim=1).tolist())

# Map predicted labels to department names
predicted_departments = [department_names[prediction] for prediction in predictions]
print(predicted_departments)


