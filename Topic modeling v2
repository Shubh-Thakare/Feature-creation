pip install transformers

pip install torch

from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import torch
model_name = "distilbert-base-uncased"  # Replace with your desired model name
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

labels = ["label1", "label2", "label3", ...]  # Replace with your list of labels
num_labels = len(labels)

topic_modeling_pipeline = pipeline(
    "text-classification",
    model=model,
    tokenizer=tokenizer,
    num_labels=num_labels
)


text = "Your input text goes here"
outputs = topic_modeling_pipeline(text)


from transformers import AutoTokenizer, AutoModelForSequenceClassification
import pandas as pd

# Load the pre-trained model
model_name = "bert-base-uncased"  # Example, you can choose a different model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# Load the topic list
topic_list = ["Topic 1", "Topic 2", "Topic 3"]  # Example, replace with your own topics

# Define the inference function
def predict_topic(comment):
    inputs = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        truncation=True,
        max_length=512,  # Adjust as per your needs
        padding="max_length",
        return_tensors="pt"
    )

    with torch.no_grad():
        logits = model(**inputs).logits

    predicted_topic = topic_list[torch.argmax(logits)]
    return predicted_topic

# Example usage
comments = ["Great service, very helpful!", "I'm not satisfied with the response time."]
topics = [predict_topic(comment) for comment in comments]

# Output the predicted topics
for comment, topic in zip(comments, topics):
    print(f"Comment: {comment}\nTopic: {topic}\n")





from transformers import AutoTokenizer, AutoModel
from sklearn.cluster import KMeans

# Step 2: Prepare the data

# Load customer comments and expected topics
customer_comments = [...]  # Your customer comments
expected_topics = [...]  # Your list of expected topics

# Step 3: Tokenization
tokenizer = AutoTokenizer.from_pretrained("model_name")  # Choose an appropriate tokenizer
tokenized_comments = tokenizer.batch_encode_plus(customer_comments, padding=True, truncation=True, return_tensors="pt")

# Step 4: Create input tensors
input_ids = tokenized_comments["input_ids"]
attention_masks = tokenized_comments["attention_mask"]

# Step 5: Load a pre-trained transformer model
model = AutoModel.from_pretrained("model_name")  # Choose an appropriate transformer model

# Step 6: Encode the input
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_masks)


encoded_comments = outputs.last_hidden_state

# Step 7: Topic modeling
kmeans = KMeans(n_clusters=len(expected_topics), random_state=42)
kmeans.fit(encoded_comments.numpy())

# Step 8: Assign topics
comment_topics = kmeans.labels_

# Print the assigned topics for each comment
for comment, topic_label in zip(customer_comments, comment_topics):
    print(f"Comment: {comment}\nTopic: {expected_topics[topic_label]}\n")
    
    





pip install transformers
pip install numpy
from transformers import pipeline
import numpy as np
classifier = pipeline("text-classification", model="nlptown/bert-base-multilingual-uncased-sentiment")

comment_topics = classifier(comments)
expected_topics = ["topic1", "topic2", "topic3"]
predicted_topics = []

for comment_topic in comment_topics:
    predicted_label = comment_topic[0]['label']
    predicted_score = comment_topic[0]['score']
    
    # Find the closest expected topic based on the predicted label
    closest_topic = min(expected_topics, key=lambda x: abs(predicted_label - x))
    predicted_topics.append(closest_topic)



