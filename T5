model.fitfrom sklearn.model_selection import train_test_split

# Split the data into training and testing sets
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

from transformers import T5Tokenizer, T5ForConditionalGeneration, T5TrainingArguments, Trainer

# Load the tokenizer and model
tokenizer = T5Tokenizer.from_pretrained("t5-base")
model = T5ForConditionalGeneration.from_pretrained("t5-base")

# Set up the training arguments
training_args = T5TrainingArguments(
    output_dir="./t5_classification_model",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir="./logs",
)

# Define a function to preprocess the data for T5 model
def preprocess_function(examples):
    inputs = tokenizer.batch_encode_plus(
        examples["comment"],
        padding="max_length",
        max_length=256,
        truncation=True,
        return_tensors="pt",
    )
    labels = tokenizer.batch_encode_plus(
        examples["department_label"],
        padding="max_length",
        max_length=16,
        truncation=True,
        return_tensors="pt",
    )
    inputs.update(labels)
    return inputs

# Preprocess the training and testing data
train_dataset = train_data.rename(columns={"comment": "input_text", "department_label": "target_text"})
train_dataset = train_dataset.apply(preprocess_function, axis=1)
eval_dataset = test_data.rename(columns={"comment": "input_text", "department_label": "target_text"})
eval_dataset = eval_dataset.apply(preprocess_function, axis=1)

# Define the training and evaluation data
def model_data_loader(dataset):
    return torch.utils.data.DataLoader(dataset, batch_size=training_args.per_device_train_batch_size)

# Fine-tune the T5 model
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=model_data_loader,
    tokenizer=tokenizer,
)
trainer.train()


# Load the trained model
trained_model = T5ForConditionalGeneration.from_pretrained("./t5_classification_model")

# Define a function to predict the department label
def predict_department(comment):
    input_text = "classify: " + comment
    inputs = tokenizer.encode_plus(
        input_text,
        padding="max_length",
        max_length=256,
        truncation=True,
        return_tensors="pt",
    )
    output = trained_model.generate(inputs["input_ids"])
    predicted_department = tokenizer.decode(output[0], skip_special_tokens=True)
    return predicted_department

# Example usage
new_comment = "This product is great!"
predicted_label = predict_department(new_comment)
print(predicted_label)










import tensorflow as tf
import tensorflow_text as text
from transformers import T5Tokenizer, TFT5ForConditionalGeneration

# Initialize the T5 tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-base')

# Tokenize the input text
train_encodings = tokenizer.batch_encode_plus(
    train_data['comment'].tolist(),
    truncation=True,
    padding=True,
    return_tensors='tf'
)

# Create TensorFlow Datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))


# Fine-tune the T5 model
model = TFT5ForConditionalGeneration.from_pretrained('t5-base')
model.compile(optimizer='adam', loss=model.compute_loss)
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3)

# Tokenize the test data
test_encodings = tokenizer.batch_encode_plus(
    test_data['comment'].tolist(),
    truncation=True,
    padding=True,
    return_tensors='tf'
)

# Create TensorFlow Datasets for test data
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

# Evaluate the model
model.evaluate(test_dataset.batch(16))


def predict_department(comment):
    input_ids = tokenizer.encode(comment, return_tensors='tf')
    outputs = model.generate(input_ids=input_ids, max_length=2)
    predicted_label = label_encoder.inverse_transform(outputs.numpy().flatten())[0]
    return predicted_label

# Example usage
new_comment = "This product is amazing!"
predicted_department = predict_department(new_comment)
print("Predicted department:", predicted_department)








import pandas as pd
from sklearn.model_selection import train_test_split
from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config
# Load the dataset into a DataFrame
df = pd.read_csv('your_dataset.csv')

# Preprocess the dataset if needed

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
# Initialize the T5 tokenizer
tokenizer = T5Tokenizer.from_pretrained('t5-base')

# Tokenize the comments
train_encodings = tokenizer.batch_encode_plus(
    train_df['comment'].tolist(),
    padding=True,
    truncation=True,
    max_length=512,
    return_attention_mask=True,
    return_tensors='pt'
)

test_encodings = tokenizer.batch_encode_plus(
    test_df['comment'].tolist(),
    padding=True,
    truncation=True,
    max_length=512,
    return_attention_mask=True,
    return_tensors='pt'
)

# Initialize the T5 model
model = T5ForConditionalGeneration.from_pretrained('t5-base')

# Configure the model
model_config = T5Config.from_pretrained('t5-base')
model_config.num_labels = len(df['department'].unique())  # Set the number of output labels

# Fine-tune the model
model.resize_token_embeddings(len(tokenizer))
model.train()

# Train the model
train_inputs = {
    'input_ids': train_encodings['input_ids'],
    'attention_mask': train_encodings['attention_mask'],
    'labels': tokenizer.batch_encode_plus(
        train_df['department'].tolist(),
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )['input_ids']
}

model.forward(**train_inputs)


# Evaluate the model
test_inputs = {
    'input_ids': test_encodings['input_ids'],
    'attention_mask': test_encodings['attention_mask'],
    'labels': tokenizer.batch_encode_plus(
        test_df['department'].tolist(),
        padding=True,
        truncation=True,
        max_length=512,
        return_tensors='pt'
    )['input_ids']
}

model.eval()
model.forward(**test_inputs)


# Preprocess the new comment
new_comment = "This is a new comment to predict the department."
new_encodings = tokenizer.batch_encode_plus(
    [new_comment],
    padding=True,
    truncation=True,
    max_length=512,
    return_attention_mask=True,
    return_tensors='pt'
)

# Predict the department for the new comment
model.eval()
with torch.no_grad():
    outputs = model.generate(
        input_ids=new_encodings['input_ids'],
        attention_mask=new_encodings['attention_mask'],
        max_length=2  # Assuming the number of departments is 2
    )

# Decode the predicted output
predicted_department = tokenizer.decode(outputs[0])
print("Predicted Department:", predicted_department)
