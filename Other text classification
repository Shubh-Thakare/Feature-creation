from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

# Step 1: Preprocessing the Text Data

# Assume 'X' is a list of preprocessed text documents
# Assume 'y' is a list of corresponding labels

# Step 2: Feature Extraction

vectorizer = CountVectorizer()  # You can replace CountVectorizer with TF-IDF or other feature extraction methods
X = vectorizer.fit_transform(X)

# Step 3: Building the Random Forest Model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)
rf_classifier.fit(X_train, y_train)

# Step 4: Evaluating the Model

y_pred = rf_classifier.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))





import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
# Load data into a DataFrame
data = pd.read_csv('your_data_file.csv')

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the training and testing data
train_encodings = tokenizer(list(train_data['comment']), truncation=True, padding=True)
test_encodings = tokenizer(list(test_data['comment']), truncation=True, padding=True)
# Create a label mapping dictionary
label_mapping = {label: idx for idx, label in enumerate(train_data['department'].unique())}

# Encode the labels
train_labels = [label_mapping[label] for label in train_data['department']]
test_labels = [label_mapping[label] for label in test_data['department']]

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))

# Load the pre-trained model
model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_mapping))

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

results = model.evaluate(test_dataset.batch(16))
print(f"Test loss: {results[0]}, Test accuracy: {results[1]}")

# Tokenize the new comment
new_comment = "This is a new comment."
new_encodings = tokenizer([new_comment], truncation=True, padding=True)

# Make predictions
predictions = model.predict(dict(new_encodings))

# Decode the predicted label
predicted_label = list(label_mapping.keys())[tf.argmax(predictions.logits, axis=1).numpy()[0]]
print(f"Predicted department: {predicted_label}")




pip install transformers
pip install torch
pip install pandas
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW

# Assuming you have a DataFrame named 'data' with 'comment' and 'department' columns
comments = data['comment'].values
labels = data['department'].values

# Split the dataset into train and test sets
train_comments, test_comments, train_labels, test_labels = train_test_split(
    comments, labels, test_size=0.2, random_state=42
)

class CustomDataset(Dataset):
    def __init__(self, comments, labels, tokenizer, max_length):
        self.comments = comments
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        comment = str(self.comments[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            comment,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=50)

train_dataset = CustomDataset(train_comments, train_labels, tokenizer, max_length=128)
test_dataset = CustomDataset(test_comments, test_labels, tokenizer, max_length=128)

batch_size = 16
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0

    for batch in train_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        train_loss += loss.item()

        loss.backward()
        optimizer.step()

    train_loss = train_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss}')

model.eval()


def predict_department(comment):
    encoding = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input
    
  def predict_department(comment):
    encoding = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(input_ids, attention_mask=attention_mask)

    predicted_label = torch.argmax(outputs.logits).item()
    return predicted_label
  







import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_comments = train_df['comment'].tolist()
train_labels = train_df['department_label'].tolist()
test_comments = test_df['comment'].tolist()
test_labels = test_df['department_label'].tolist()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)

train_encodings = tokenizer(train_comments, truncation=True, padding=True)
test_encodings = tokenizer(test_comments, truncation=True, padding=True)

train_labels = torch.tensor(train_labels)
test_labels = torch.tensor(test_labels)

class CommentDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CommentDataset(train_encodings, train_labels)
test_dataset = CommentDataset(test_encodings, test_labels)

train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)

model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=50)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

optimizer = AdamW(model.parameters(), lr=1e-5)

for epoch in range(3):  # Adjust the number of epochs as needed
    model.train()
    for batch in train_loader:
        optimizer.zero_grad()
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()

model.eval()

def predict_department(comment):
    inputs = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        max_length=512,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )
    inputs = inputs.to(device)
    outputs = model(**inputs)
    predicted_label = torch.argmax(outputs.logits, dim=1).item()
    return predicted_label

new_comment = "This is a new comment. Please predict the department."
predicted_department = predict_department(new_comment)
print(predicted_department)










import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW
from torch.utils.data import DataLoader, Dataset
import torch
# Assuming you have your dataset stored in a CSV file
df = pd.read_csv('your_dataset.csv')

# Split the dataset into training and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# Define the column names for comment and department
comment_column = 'customer_comment'
department_column = 'department_label'

# Convert the labels to numerical values
labels = train_df[department_column].unique().tolist()
label_to_id = {label: i for i, label in enumerate(labels)}
id_to_label = {i: label for label, i in label_to_id.items()}

train_df['label'] = train_df[department_column].map(label_to_id)
test_df['label'] = test_df[department_column].map(label_to_id)

class CustomDataset(Dataset):
    def __init__(self, dataframe, tokenizer, max_length):
        self.data = dataframe
        self.comment = self.data[comment_column]
        self.targets = self.data['label']
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        comment = str(self.comment[index])
        comment = ' '.join(comment.split())  # Remove extra whitespaces

        inputs = self.tokenizer.encode_plus(
            comment,
            None,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_token_type_ids=True
        )

        input_ids = inputs['input_ids']
        attention_mask = inputs['attention_mask']
        token_type_ids = inputs['token_type_ids']

        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),
            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),
            'targets': torch.tensor(self.targets[index], dtype=torch.long)
        }

MAX_LENGTH = 128  # Maximum sequence length

tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(labels))

BATCH_SIZE = 16  # Adjust according to your available memory

train_dataset = CustomDataset(train_df, tokenizer, MAX_LENGTH)
test_dataset = CustomDataset(test_df, tokenizer, MAX_LENGTH)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)

EPOCHS = 3  # Number of training epochs
LEARNING_RATE = 2e-5

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)

optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
    model.train()

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        targets = batch['targets'].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            labels=targets
        )

        loss = outputs.loss
        logits = outputs.logits

        loss.backward()
        optimizer.step()

    print(f'Epoch {epoch + 1}/{EPOCHS} - Loss: {loss.item()}')


model.eval()

predictions = []
true_labels = []

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        token_type_ids = batch['token_type_ids'].to(device)
        targets = batch['targets'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            labels=targets
        )

        _, predicted_labels = torch.max(outputs.logits, dim=1)

        predictions.extend(predicted_labels.detach().cpu().numpy().tolist())
        true_labels.extend(targets.detach().cpu().numpy().tolist())

accuracy = accuracy_score(true_labels, predictions)
classification_report = classification_report(true_labels, predictions, target_names=labels)

print(f'Accuracy: {accuracy}')
print(f'Classification Report:\n{classification_report}')

## Make predictions on new comments:
def predict_department(comment):
    model.eval()

    inputs = tokenizer.encode_plus(
        comment,
        None,
        add_special_tokens=True,
        max_length=MAX_LENGTH,
        padding='max_length',
        truncation=True,
        return_token_type_ids=True
    )

    input_ids = torch.tensor(inputs['input_ids'], dtype=torch.long).unsqueeze(0).to(device)
    attention_mask = torch.tensor(inputs['attention_mask'], dtype=torch.long).unsqueeze(0).to(device)
    token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long).unsqueeze(0).to(device)

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids
        )

        _, predicted_label = torch.max(outputs.logits, dim=1)

    predicted_label = predicted_label.item()
    predicted_department = id_to_label[predicted_label]

    return predicted_department


