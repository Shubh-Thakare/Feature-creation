from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split

# Step 1: Preprocessing the Text Data

# Assume 'X' is a list of preprocessed text documents
# Assume 'y' is a list of corresponding labels

# Step 2: Feature Extraction

vectorizer = CountVectorizer()  # You can replace CountVectorizer with TF-IDF or other feature extraction methods
X = vectorizer.fit_transform(X)

# Step 3: Building the Random Forest Model

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10)
rf_classifier.fit(X_train, y_train)

# Step 4: Evaluating the Model

y_pred = rf_classifier.predict(X_test)

print("Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))





import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from transformers import TFAutoModelForSequenceClassification, AutoTokenizer
# Load data into a DataFrame
data = pd.read_csv('your_data_file.csv')

# Split the data into training and testing sets
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the training and testing data
train_encodings = tokenizer(list(train_data['comment']), truncation=True, padding=True)
test_encodings = tokenizer(list(test_data['comment']), truncation=True, padding=True)
# Create a label mapping dictionary
label_mapping = {label: idx for idx, label in enumerate(train_data['department'].unique())}

# Encode the labels
train_labels = [label_mapping[label] for label in train_data['department']]
test_labels = [label_mapping[label] for label in test_data['department']]

train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels))

# Load the pre-trained model
model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_mapping))

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

results = model.evaluate(test_dataset.batch(16))
print(f"Test loss: {results[0]}, Test accuracy: {results[1]}")

# Tokenize the new comment
new_comment = "This is a new comment."
new_encodings = tokenizer([new_comment], truncation=True, padding=True)

# Make predictions
predictions = model.predict(dict(new_encodings))

# Decode the predicted label
predicted_label = list(label_mapping.keys())[tf.argmax(predictions.logits, axis=1).numpy()[0]]
print(f"Predicted department: {predicted_label}")




pip install transformers
pip install torch
pip install pandas
import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertForSequenceClassification, AdamW

# Assuming you have a DataFrame named 'data' with 'comment' and 'department' columns
comments = data['comment'].values
labels = data['department'].values

# Split the dataset into train and test sets
train_comments, test_comments, train_labels, test_labels = train_test_split(
    comments, labels, test_size=0.2, random_state=42
)

class CustomDataset(Dataset):
    def __init__(self, comments, labels, tokenizer, max_length):
        self.comments = comments
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        comment = str(self.comments[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            comment,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt',
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=50)

train_dataset = CustomDataset(train_comments, train_labels, tokenizer, max_length=128)
test_dataset = CustomDataset(test_comments, test_labels, tokenizer, max_length=128)

batch_size = 16
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

optimizer = AdamW(model.parameters(), lr=2e-5)

num_epochs = 5
for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0

    for batch in train_dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()

        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        train_loss += loss.item()

        loss.backward()
        optimizer.step()

    train_loss = train_loss / len(train_dataloader)
    print(f'Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss}')

model.eval()


def predict_department(comment):
    encoding = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        max_length=128,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input
