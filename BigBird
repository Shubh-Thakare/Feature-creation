import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from transformers import BigBirdTokenizer, TFBigBirdForSequenceClassification
import tensorflow as tf

# Load the data into a Pandas DataFrame
df = pd.read_csv('data.csv')  # Replace 'data.csv' with your actual data file

# Split the data into training and testing sets
train_texts, test_texts, train_labels, test_labels = train_test_split(df['comment'], df['department'], test_size=0.2, random_state=42)

tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')

train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)
test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True)

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels.tolist()
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels.tolist()
))

model = TFBigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base', num_labels=len(df['department'].unique()))
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

_, test_acc = model.evaluate(test_dataset.batch(16))
print('Test accuracy:', test_acc)

def predict_department(comment):
    input_ids = tokenizer.encode(comment, truncation=True, padding=True, max_length=512, return_tensors='tf')
    logits = model.predict(input_ids)[0]
    predicted_class = np.argmax(logits)
    predicted_label = df['department'].unique()[predicted_class]
    return predicted_label

# Example usage:
new_comment = "This is a new comment."
predicted_department = predict_department(new_comment)
print('Predicted department:', predicted_department)







pip install tensorflow tensorflow-text transformers
import tensorflow as tf
import tensorflow_text as text
from transformers import BigBirdTokenizer, TFBigBirdForSequenceClassification
tokenizer = BigBirdTokenizer.from_pretrained('google/bigbird-roberta-base')
def prepare_input_data(texts, labels):
    input_ids = []
    attention_masks = []

    for text in texts:
        encoded_text = tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=128,
            pad_to_max_length=True,
            truncation=True,
            return_attention_mask=True
        )
        input_ids.append(encoded_text['input_ids'])
        attention_masks.append(encoded_text['attention_mask'])

    return np.array(input_ids), np.array(attention_masks), np.array(labels)

def build_model():
    input_ids = tf.keras.Input(shape=(128,), dtype=tf.int32)
    attention_mask = tf.keras.Input(shape=(128,), dtype=tf.int32)
    model = TFBigBirdForSequenceClassification.from_pretrained('google/bigbird-roberta-base')
    logits = model(input_ids, attention_mask=attention_mask)[0]
    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(logits)

    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=outputs)
    return model

# Define model hyperparameters
num_classes = len(df['department_label'].unique())
learning_rate = 1e-5
epsilon = 1e-08
epochs = 5
batch_size = 32

# Build and compile the model
model = build_model()
optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)
model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Prepare input data
train_texts, train_labels = df_train['comment'].values, df_train['department_label'].values
train_input_ids, train_attention_masks, train_labels = prepare_input_data(train_texts, train_labels)

# Train the model
model.fit(
    [train_input_ids, train_attention_masks],
    train_labels,
    epochs=epochs,
    batch_size=batch_size,
    validation_split=0.2
)

def predict_department(comment):
    input_ids, attention_mask, _ = prepare_input_data([comment], [0])
    prediction = model.predict([input_ids, attention_mask])[0]
    predicted_label = np.argmax(prediction)
    return predicted_label

new_comment = "This is a new comment. What department does it belong to?"
predicted_department = predict_department(new_comment)








import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BigBirdForSequenceClassification, BigBirdTokenizer
class CommentDataset(Dataset):
    def __init__(self, comments, labels, tokenizer, max_length):
        self.comments = comments
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.comments)

    def __getitem__(self, idx):
        comment = str(self.comments[idx])
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            comment,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].squeeze()
        attention_mask = encoding['attention_mask'].squeeze()

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'label': torch.tensor(label, dtype=torch.long)
        }


model_name = 'google/bigbird-base-uncased'
model = BigBirdForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
tokenizer = BigBirdTokenizer.from_pretrained(model_name)

train_dataset = CommentDataset(train_comments, train_labels, tokenizer, max_length)
test_dataset = CommentDataset(test_comments, test_labels, tokenizer, max_length)

batch_size = 16
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

epochs = 5
for epoch in range(epochs):
    model.train()
    total_loss = 0

    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )

        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    average_loss = total_loss / len(train_loader)
    print(f'Epoch {epoch + 1}/{epochs} - Loss: {average_loss:.4f}')

model.eval()
correct = 0
total = 0

with torch.no_grad():
    for batch in test_loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['label'].to(device)

        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

        _, predicted = torch.max(outputs.logits, dim=1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

accuracy = 100 * correct / total
print(f'Test Accuracy: {accuracy:.2f}%')

def predict_department(comment):
    encoding = tokenizer.encode_plus(
        comment,
        add_special_tokens=True,
        max_length=max_length,
        padding='max_length',
        truncation=True,
        return_tensors='pt'
    )

    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)

    with torch.no_grad():
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask
        )

    _, predicted = torch.max(outputs.logits, dim=1)

    return predicted.item()

new_comment = "This is a new comment."
predicted_department = predict_department(new_comment)
print(f'Predicted Department: {predicted_department}')

