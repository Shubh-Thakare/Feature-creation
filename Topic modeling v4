import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from transformers import pipeline

def get_token_counts(text_data):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(text_data)
    feature_names = vectorizer.get_feature_names()
    return X, feature_names

def apply_topic_modeling(X, num_topics):
    lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)
    lda.fit(X)
    return lda

def extract_topics(lda, feature_names, num_words):
    topics = []
    for topic_idx, topic in enumerate(lda.components_):
        topic_words = [feature_names[i] for i in topic.argsort()[:-num_words - 1:-1]]
        topics.append(topic_words)
    return topics

def apply_topic_labeling(topics):
    nlp = pipeline("text-classification", model="nlptown/bert-base-multilingual-uncased-sentiment")
    labeled_topics = []
    for topic in topics:
        topic_label = nlp(" ".join(topic))[0]
        labeled_topics.append((topic_label['label'], topic))
    return labeled_topics

def topic_modeling_pipeline(text_data, num_topics, num_words):
    X, feature_names = get_token_counts(text_data)
    lda = apply_topic_modeling(X, num_topics)
    topics = extract_topics(lda, feature_names, num_words)
    labeled_topics = apply_topic_labeling(topics)
    return labeled_topics





import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from transformers import pipeline

nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Remove punctuation and convert to lowercase
    text = text.lower().replace('[^\w\s]', '')
    # Tokenize the text
    tokens = word_tokenize(text)
    # Remove stopwords
    tokens = [token for token in tokens if token not in stop_words]
    return ' '.join(tokens)

df['processed_text'] = df['feedback_text'].apply(preprocess_text)
vectorizer = CountVectorizer(max_features=1000)
X = vectorizer.fit_transform(df['processed_text'])
n_topics = 5  # Define the number of topics to extract
lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)
lda.fit(X)

feature_names = vectorizer.get_feature_names()

for topic_idx, topic in enumerate(lda.components_):
    print(f"Topic #{topic_idx + 1}:")
    top_words = [feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]
    print(top_words)

classifier = pipeline('text-classification', model='distilbert-base-uncased', tokenizer='distilbert-base-uncased')

# Example call center feedback text
feedback_text = "I am having trouble with my internet connection."

# Classify the feedback into a topic
result = classifier(feedback_text)

print(result['label'])  # Print the predicted topic label
print(result['score'])  # Print the confidence score







pip install gensim
pip install nltk
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from gensim import models, corpora

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Preprocessing function
def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())
  
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token for token in tokens if token not in stop_words]
  
    # Lemmatize the tokens
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]
  
    return lemmatized_tokens

# Preprocess the data
preprocessed_data = [preprocess_text(text) for text in call_center_feedbacks]

# Create a dictionary
dictionary = corpora.Dictionary(preprocessed_data)

# Create a corpus
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_data]

# Train the LDA model
num_topics = 5
lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)

# Analyze the topics
for topic_num in range(num_topics):
    print(f"Topic #{topic_num + 1}:")
    print(lda_model.print_topic(topic_num))
    print()
