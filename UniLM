import torch
from transformers import UniLMTokenizer, UniLMForSequenceClassification

comments = df['comment'].tolist()
labels = df['department_label'].tolist()

tokenizer = UniLMTokenizer.from_pretrained('unilm-base-cased')
encoded_inputs = tokenizer(comments, truncation=True, padding=True, return_tensors='pt')

label_map = {label: i for i, label in enumerate(set(labels))}
encoded_labels = [label_map[label] for label in labels]

from sklearn.model_selection import train_test_split

input_ids = encoded_inputs['input_ids']
attention_mask = encoded_inputs['attention_mask']

train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, encoded_labels, test_size=0.2, random_state=42)
train_masks, val_masks, _, _ = train_test_split(attention_mask, input_ids, test_size=0.2, random_state=42)

model = UniLMForSequenceClassification.from_pretrained('unilm-base-cased', num_labels=len(label_map))

batch_size = 16
num_epochs = 5
learning_rate = 2e-5
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

from torch.utils.data import TensorDataset, DataLoader
train_dataset = TensorDataset(train_inputs, train_masks, train_labels)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

val_dataset = TensorDataset(val_inputs, val_masks, val_labels)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for batch in train_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        total_loss += loss.item()

        loss.backward()
        optimizer.step()

    avg_train_loss = total_loss / len(train_dataloader)

    model.eval()
    val_loss = 0
    num_val_steps = 0

    for batch in val_dataloader:
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2].to(device)

        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            val_loss += loss.item()
            num_val_steps += 1

    avg_val_loss = val_loss / num_val_steps

    print(f'Epoch {epoch+1}/{num_epochs} - Avg. Train Loss: {avg_train_loss:.4f} - Avg. Val Loss: {avg_val_loss:.4f}')

