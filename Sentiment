
import pandas as pd
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer

# Example preprocessing using NLTK
nltk.download('stopwords')
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation or special characters
    text = ''.join(c for c in text if c.isalnum() or c.isspace())
    # Remove stopwords
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text

df['preprocessed_text'] = df['text_column'].apply(preprocess_text)


sia = SentimentIntensityAnalyzer()

def get_sentiment_score(text):
    sentiment = sia.polarity_scores(text)
    return sentiment['compound']

df['sentiment_score'] = df['preprocessed_text'].apply(get_sentiment_score)


def get_sentiment(sentiment_score):
    if sentiment_score >= 0.05:
        return 'Positive'
    elif sentiment_score <= -0.05:
        return 'Negative'
    else:
        return 'Neutral'

df['sentiment'] = df['sentiment_score'].apply(get_sentiment)


# Example: Count the number of positive, negative, and neutral sentiments
sentiment_counts = df['sentiment'].value_counts()
print(sentiment_counts)





import pandas as pd
import nltk
nltk.download('punkt')  # Download the Punkt tokenizer if not already downloaded
from nltk.tokenize import sent_tokenize

data = {'comments': ['This is the first comment. It contains multiple sentences. Second sentence is here.',
                     'Another comment with a single sentence.',
                     'A third comment with even more sentences. This is the third sentence.']}
df = pd.DataFrame(data)

def split_comments(df):
    new_rows = []
    for index, row in df.iterrows():
        comment = row['comments']
        sentences = sent_tokenize(comment)
        for sentence in sentences:
            new_row = row.copy()
            new_row['comments'] = sentence
            new_rows.append(new_row)
    return pd.DataFrame(new_rows)

new_df = split_comments(df)
