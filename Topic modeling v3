# Step 1: Data Preprocessing

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenize the text
    tokens = word_tokenize(text.lower())

    # Remove stopwords and punctuation
    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]

    return tokens

# Step 2: Data Preparation

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def tokenize_text(text):
    tokens = tokenizer.encode(text, add_special_tokens=True)
    return tokens

# Step 3: Model Training

from transformers import BertForSequenceClassification
import torch

# Load pre-trained BERT model
model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

# Fine-tune the model on your dataset
# ...

# Step 4: Topic Inference

def get_document_embeddings(text):
    tokens = tokenize_text(text)
    input_ids = torch.tensor(tokens).unsqueeze(0)

    # Get BERT model outputs
    outputs = model(input_ids)

    # Extract hidden representations (embeddings) from the model
    embeddings = outputs.last_hidden_state.squeeze(0)

    return embeddings

# Step 5: Clustering or Topic Extraction

from sklearn.cluster import KMeans

def cluster_documents(document_embeddings, num_clusters):
    # Apply k-means clustering
    kmeans = KMeans(n_clusters=num_clusters, random_state=0)
    clusters







pip install transformers nltk numpy
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('stopwords')

def preprocess_text(documents):
    # Convert to lowercase
    documents = [doc.lower() for doc in documents]
    
    # Tokenize the documents
    tokenized_docs = [word_tokenize(doc) for doc in documents]
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokenized_docs = [[word for word in doc if word not in stop_words] for doc in tokenized_docs]
    
    return tokenized_docs
    

from transformers import BertTokenizer

def vectorize_text(tokenized_docs):
    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
    tokenized_docs = [' '.join(doc) for doc in tokenized_docs]
    
    # Tokenize and encode the documents
    encoded_docs = tokenizer(tokenized_docs, padding=True, truncation=True, return_tensors='pt')
    
    return encoded_docs

import torch
from sklearn.cluster import KMeans

def topic_modeling(encoded_docs):
    # Load pre-trained BERT model
    model = torch.hub.load('huggingface/pytorch-transformers', 'modelForSequenceClassification', 'bert-base-uncased')
    
    # Obtain document embeddings using BERT
    with torch.no_grad():
        outputs = model(**encoded_docs)
        embeddings = outputs[0][:, 0, :].numpy()  # Use the [CLS] token embedding
    
    # Apply clustering to obtain topics
    num_topics = 5  # Specify the desired number of topics
    kmeans = KMeans(n_clusters=num_topics)
    kmeans.fit(embeddings)


    
    # Return the cluster labels
    return kmeans.labels_
    
    
    
    
    
    
model_name = "bert-base-nli-mean-tokens"
model = pipeline("feature-extraction", model=model_name)
documents = [
    "Document 1 text goes here.",
    "Document 2 text goes here.",
    "Document 3 text goes here.",
    # Add more documents as needed
]
document_vectors = model(documents)
from sklearn.cluster import KMeans

k = 5  # Number of clusters
kmeans = KMeans(n_clusters=k)
clusters = kmeans.fit_predict(document_vectors)
for i, cluster_id in enumerate(clusters):
    print(f"Document {i+1} belongs to cluster {cluster_id+1}")

